[{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributor’s Guide","title":"Contributor’s Guide","text":"document help set access main repository harsat code, help contribute code.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"summary","dir":"","previous_headings":"","what":"Summary","title":"Contributor’s Guide","text":"Main repository Github: https://github.com/osparcomm/HARSAT Repository currently private – access permissions, contact Chris Moulton OSPAR team, ones administer repository Github. need access read code contribute. Web documentation Github Pages: https://osparcomm.github.io/HARSAT/","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"packaging","dir":"","previous_headings":"","what":"Packaging","title":"Contributor’s Guide","text":"aim make harsat code work R package. going distributed CRAN near future least. Instead, can installed directly Github. install latest development version, use remotes package: Installing latest stable version similar, less likely change break. web documentation reflects stable version, development version. install harsat code dependencies. Note: development repository marked private GitHub, need GitHub Personal Access Token (PAT) access . Put call ‘XXXX’ string.","code":"library(remotes) remotes::install_github(\"osparcomm/harsat\", auth_token = 'XXXX') library(remotes) remotes::install_github(\"osparcomm/harsat@main\", auth_token = 'XXXX')"},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"Contributing","what":"Issues","title":"Contributor’s Guide","text":"can report issues harsat using Github. Simply use Issues tab choose New issue. report issue, ’d like answer following questions. version harsat using? (’s DESCRIPTION file) version R using? operating system using? observing appears incorrect, expecting see? last point, information can give us, better. can copy paste R console logs helps tremendously, ’re 30 lines , ’s better attach file, rather pasting issue directly.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"Contributing","what":"Pull requests","title":"Contributor’s Guide","text":"welcome pull requests. easiest way fork repository Github, make changes want fork, push Github, create pull request using web interface. point show main repository can collaborate integrate code. Please note particularly keen improve code quality.","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Contributor’s Guide","text":"documentation held within Github repository. use following flow. roxygen2 used source-code documentation. particularly welcome pull requests improve documentation. several vignettes vignettes directory. actually run harsat code (ones matching *.Rmd.orig) therefore precompiled, can take 15-20 minutes run. turns *.Rmd.orig corresponding *.Rmd files. , normal documentation building vignettes installation deploy files, along basic *.Rmd vignettes run harsat code. web site built using pkgdown – happens automatically pull requests merged main. documents code – run roxygen2 – need , manually. merge pull requests, web documentation used update GitHub Pages site: https://osparcomm.github.io/HARSAT/","code":""},{"path":"http://osparcomm.github.io/HARSAT/CONTRIBUTING.html","id":"documentation-build-process","dir":"","previous_headings":"Documentation","what":"Documentation build process","title":"Contributor’s Guide","text":"documentation entirely built automatically. intentional, , example, running tools (e.g., roxygen2) can change key package files breaking way. Also, vignettes pre-rendered, take long time install users. update documentation, use following steps R. change namespacing, API changes can cause vignette builds break. , create issue. Step 1. Run roxygen2 Step 2. Precompile vignettes bash: Step 3. Build check code Back R:","code":"library(devtools) roxygen2::roxygenize() R --quiet --vanilla < vignettes/precompile.R library(devtools) devtools::check()"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_HELCOM.html","id":"sediment-assessment","dir":"Articles","previous_headings":"","what":"Sediment assessment","title":"HELCOM","text":"sediment assessment similar, extra features related normalisation (account differences grain size) describe create_timeseries call sediment differs call water two ways. First, determinands.control identifies two groups determinands need summed. Second, arguments normalise normalise.control specify normalisation grain size carried . default functions normalisation work many cases. However, process HELCOM complicated (unlike metals, copper normalised organic carbon) customised function normalise_sediment_HELCOM provided. argument normalise.control specifies metals (apart copper) normalised 5% aluminium copper organics normalised 5% organic carbon. normalise functions need bit work, expect change. Now run assessment. one threshold, EQS. takes minute run laptop. Everything converged time. Finally, can plot individual time series assessments (see vignette external data) print summary table","code":"sediment_data <- read_data(   compartment = \"sediment\",   purpose = \"HELCOM\",   contaminants = \"sediment.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_HELCOM\"),   info_dir = file.path(working.directory, \"information\", \"HELCOM_2023\"),   extraction = \"2023/08/23\" ) #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/HELCOM_2023/determinand.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/HELCOM_2023/species.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in path thresholds_sediment.csv /Users/stuart/git/HARSAT/information/HELCOM_2023/thresholds_sediment.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_HELCOM/stations.txt' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_HELCOM/sediment.txt' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: HELCOM  #>  - no restriction of stations by data type #>  - no restriction of stations by purpose of monitoring #>  - no restriction of data by program governance #>  - no restriction of stations by program governance #>  - matching 6995 records by coordinates #>  - matching 9365 records by station name #>  - 15721 of 16360 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2020  sediment_data <- tidy_data(sediment_data) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Dropping 2176 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth > 0m #>  - unusual matrix #>  #> Dropping 523 records from data that have no valid station code #>  #> Dropping 13326 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data sediment_timeseries <- create_timeseries(   sediment_data,   determinands.control = list(     SBDE6 = list(       det = c(\"BDE28\", \"BDE47\", \"BDE99\", \"BD100\", \"BD153\", \"BD154\"),        action = \"sum\"     ),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\")   ),   normalise = normalise_sediment_HELCOM,   normalise.control = list(     metals = list(method = \"pivot\", normaliser = \"AL\", value = 5),      copper = list(method = \"hybrid\", normaliser = \"CORG\", value = 5),     organics = list(method = \"simple\", normaliser = \"CORG\", value = 5)    ) ) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2015 and 2020  #>    Dropping samples with only auxiliary variables #>    Relabelling matrix SED62 as SED63 and SED500, SED1000, SED2000 as SEDTOT #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Non-positive quantification limits: see non_positive_quant_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Non-positive uncertainties: see non_positive_uncertainties.csv #>    Large uncertainties: see large_uncertainties.csv #>    Data submitted as BDE28, BDE47, BDE99, BD100, BD153, BD154 summed to give  #> SBDE6 #>      61 of 124 samples lost due to incomplete submissions #>    Data submitted as HBCDA, HBCDB, HBCDG summed to give HBCD #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Losing 6 out of 3282 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Normalising copper to CORG using pivot values #>    Removing sediment data where normaliser is a less than #>    Normalising metals to AL using pivot values #>    Normalising organics to 5% CORG #>    Removing sediment data where normaliser is a less than #>    Dropping groups of compounds / stations with no data between 2015 and 2020 sediment_assessment <- run_assessment(   sediment_timeseries,    AC = \"EQS\",   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(sediment_assessment) #> All assessment models have converged write_summary_table(   sediment_assessment,   determinandGroups = webGroups <- list(     levels = c(\"Metals\", \"Organotins\", \"PAH_parent\", \"PBDEs\", \"Organobromines\"),       labels = c(       \"Metals\", \"Organotins\", \"Polycyclic aromatic hydrocarbons\",         \"Organobromines\", \"Organobromines\"      )   ),   classColour = list(     below = c(\"EQS\" = \"green\"),      above = c(\"EQS\" = \"red\"),      none = \"black\"   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_HELCOM.html","id":"biota-assessment","dir":"Articles","previous_headings":"","what":"Biota assessment","title":"HELCOM","text":"main difference biota assessment inclusion effects data. example PAH metabolite time series, imposex data excluded keep things relatively simple. Imposex assessments additional modelling stage described another vignette (yet available). first two stages just construction time series features. However, first need provide individual TEQs allow construction TEQ dioxins, furans planar PCBS (labelled TEQDFP). values human health QS. stage won’t necessary later releases. determinands.control argument rather . four summed variables: PFOS, SBDE6, HBCD SCB6. also one variable CB138+163 needs relabeled (replaced ) CB138. purposes assessment, contribution CB163 regarded small, CB138+163 taken good proxy CB138. Note replacements must done six PCBs summed give SCB6 order included sum. also two ‘bespoke’ actions determinands.control. customised functions complicated (non-standard things). One computes TEQDFP. deals three different ways lipid weight measurements can submitted. Finally, normalise_biota_HELCOM() customised function determines measurements normalised 5% lipid HELCOM assessment. , normalisation functions active development might well change first release. asssessment took 3.5 minutes laptop ’s :)","code":"biota_data <- read_data(   compartment = \"biota\",    purpose = \"HELCOM\",                                  contaminants = \"biota.txt\",    stations = \"stations.txt\",    data_dir = file.path(working.directory, \"data\", \"example_HELCOM\"),   info_dir = file.path(working.directory, \"information\", \"HELCOM_2023\"),    extraction = \"2023/08/23\" ) #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/HELCOM_2023/determinand.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/HELCOM_2023/species.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in path thresholds_biota.csv /Users/stuart/git/HARSAT/information/HELCOM_2023/thresholds_biota.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/HELCOM_2023\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_HELCOM/stations.txt' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_HELCOM/biota.txt' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: HELCOM  #>  - no restriction of stations by data type #>  - no restriction of stations by purpose of monitoring #>  - no restriction of data by program governance #>  - no restriction of stations by program governance #>  - matching 8644 records by coordinates #>  - matching 19893 records by station name #>  - 28437 of 28537 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2020  biota_data <- tidy_data(biota_data) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Dropping 32 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - species missing #>  #> Dropping 99 records from data that have no valid station code #>  #> Dropping 13664 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data info_TEQ <- c(   \"CB77\" = 0.0001, \"CB81\" = 0.0003, \"CB105\" = 0.00003, \"CB118\" = 0.00003,    \"CB126\" = 0.1, \"CB156\" = 0.00003, \"CB157\" = 0.00003, \"CB167\" = 0.00003,    \"CB169\" = 0.03, \"CDD1N\" = 1, \"CDD4X\" = 0.1, \"CDD6P\" = 0.01, \"CDD6X\" = 0.1,    \"CDD9X\" = 0.1, \"CDDO\" = 0.0003, \"CDF2N\" = 0.3, \"CDF2T\" = 0.1, \"CDF4X\" = 0.1,    \"CDF6P\" = 0.01, \"CDF6X\" = 0.1, \"CDF9P\" = 0.01,   \"CDF9X\" = 0.1, \"CDFO\" = 0.0003, \"CDFP2\" = 0.03, \"CDFX1\" = 0.1, \"TCDD\" = 1 ) biota_timeseries <- create_timeseries(   biota_data,   determinands.control = list(     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     SBDE6 = list(       det = c(\"BDE28\", \"BDE47\", \"BDE99\", \"BD100\", \"BD153\", \"BD154\"),        action = \"sum\"     ),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\"),     CB138 = list(det = \"CB138+163\", action = \"replace\"),     SCB6 = list(       det = c(\"CB28\", \"CB52\", \"CB101\", \"CB138\", \"CB153\", \"CB180\"),        action = \"sum\"     ),     TEQDFP = list(det = names(info_TEQ), action = \"bespoke\"),     \"LIPIDWT%\" = list(det = c(\"EXLIP%\", \"FATWT%\"), action = \"bespoke\")   ),   normalise = normalise_biota_HELCOM,   normalise.control = list(     lipid = list(method = \"simple\", value = 5),      other = list(method = \"none\")    ) ) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2015 and 2020 #>    Unexpected or missing values for 'species_group': see species_group_queries.csv #>    Unexpected or missing values for 'matrix': see matrix_queries.csv #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Bile metabolite units changed from 'ng/g' to 'ng/ml' and from 'ug/kg' to 'ug/l' #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Non-positive quantification limits: see non_positive_quant_limits.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Non-positive uncertainties: see non_positive_uncertainties.csv #>    Large uncertainties: see large_uncertainties.csv #>    Data submitted as BDE28, BDE47, BDE99, BD100, BD153, BD154 summed to give  #> SBDE6 #>      103 of 314 samples lost due to incomplete submissions #>    Data submitted as HBCDA, HBCDB, HBCDG summed to give HBCD #>      5 of 41 samples lost due to incomplete submissions #>    Data submitted as CB138+163 relabelled as CB138  #>    Data submitted as CB28, CB52, CB101, CB138, CB153, CB180 summed to give SCB6 #>      16 of 521 samples lost due to incomplete submissions #>    Data submitted as  #> CB77, CB81, CB105, CB118, CB126, CB156, CB157, CB167, CB169, CDD1N, CDD4X, CDD6P, CDD6X, CDD9X, CDDO, CDF2N, CDF2T, CDF4X, CDF6P, CDF6X, CDF9P, CDF9X, CDFO, CDFP2, CDFX1, TCDD  #> summed to give TEQDFP #>      508 of 530 samples lost due to incomplete submissions #>    Data submitted as EXLIP% or FATWT% relabelled as LIPIDWT%  #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Losing 23 out of 3030 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Normalising lipid to 5% #>    No normalisation for other #>    Dropping groups of compounds / stations with no data between 2015 and 2020 biota_assessment <- run_assessment(   biota_timeseries,    AC = c(\"BAC\", \"EAC\", \"EQS\", \"MPC\"),   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while!  check_assessment(biota_assessment) #> All assessment models have converged write_summary_table(   biota_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"PAH_parent\", \"Metabolites\", \"PBDEs\", \"Organobromines\",        \"Organofluorines\", \"Chlorobiphenyls\", \"Dioxins\"     ),       labels = c(       \"Metals\", \"PAH compounds and metabolites\", \"PAH compounds and metabolites\",       \"Organobromines\", \"Organobromines\", \"Organofluorines\",        \"PCBs and dioxins\", \"PCBs and dioxins\"     )   ),   classColour = list(     below = c(\"BAC\" = \"green\", \"EAC\" = \"green\", \"EQS\" = \"green\", \"MPC\" = \"green\"),     above = c(\"BAC\" = \"red\", \"EAC\" = \"red\", \"EQS\" = \"red\", \"MPC\" = \"red\"),     none = \"black\"   ),   collapse_AC = list(EAC = c(\"EAC\", \"EQS\", \"MPC\")),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"intro","dir":"Articles","previous_headings":"","what":"Intro","title":"HELCOM","text":"get , need set environment. First, load harsat library (let’s assume already installed , covered Getting Started guide). load R package , use working directory find data files. use data files, need point directory containing copy. Now, let’s take look water_summary.csv file:","code":"library(harsat) library(here) #> here() starts at /Users/stuart/git/HARSAT working.directory <- here() water_data <- read_data(   compartment = \"water\",   purpose = \"OSPAR\",   contaminants = \"water.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),   extraction = \"2023/08/23\" ) #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path thresholds_water.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/thresholds_water.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/stations.txt' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/water.txt' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CW or EW  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 2768 records by coordinates #>  - matching 1583 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 4251 of 4351 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2022  water_data <- tidy_data(water_data) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Dropping 411 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth >5.5m #>  - filtration missing #>  - matrix = 'SPM' #>  #> Dropping 94 records from data that have no valid station code #>  #> Dropping 13633 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data water_timeseries <- create_timeseries(   water_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\"),      HCH = list(det = c(\"HCHA\", \"HCHB\", \"HCHG\"), action = \"sum\")   ) ) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2017 and 2022 #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Data submitted as CHRTR relabelled as CHR  #>    Data submitted as BBF, BKF summed to give BBKF #>      1 of 71 samples lost due to incomplete submissions #>    Data submitted as BBJF, BKF summed to give BBJKF #>      99 of 99 samples lost due to incomplete submissions #>    Data submitted as HCEPC, HCEPT summed to give HCEPX #>    Data submitted as HCHA, HCHB, HCHG summed to give HCH #>      190 of 234 samples lost due to incomplete submissions #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Dropping groups of compounds / stations with no data between 2017 and 2022 water_assessment <- run_assessment(   water_timeseries,    AC = \"EQS\",    parallel = TRUE ) #> Setting up clusters: be patient, it can take a while!  check_assessment(water_assessment) #> All assessment models have converged summary.dir <- file.path(working.directory, \"output\", \"example_OSPAR\") if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) } write_summary_table(   water_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\", \"PAH_parent\",  \"Organofluorines\",        \"Chlorobiphenyls\", \"Organochlorines\", \"Pesticides\"     ),       labels = c(       \"Metals\", \"Organotins\", \"PAH parent compounds\", \"Organofluorines\",        \"Polychlorinated biphenyls\", \"Organochlorines (other)\", \"Pesticides\"     )   ),   classColour = list(     below = c(\"EQS\" = \"green\"),      above = c(\"EQS\" = \"red\"),      none = \"black\"   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"sediment","dir":"Articles","previous_headings":"","what":"Sediment","title":"HELCOM","text":"Now, let’s take look sediment_summary.csv file:","code":"sediment_data <- read_data(   compartment = \"sediment\",    purpose = \"OSPAR\",   contaminants = \"sediment.txt\",    stations = \"stations.txt\",    data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),    extraction = \"2023/08/23\" )   #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path thresholds_sediment.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/thresholds_sediment.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/stations.txt' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/sediment.txt' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CS or ES  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 6717 records by coordinates #>  - matching 11760 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 18054 of 18477 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2021 sediment_data <- tidy_data(sediment_data) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Dropping 75 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth > 0m #>  - unusual matrix #>  #> Dropping 423 records from data that have no valid station code #>  #> Dropping 13555 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data info_TEQ <- c(   \"CB77\" = 0.0001, \"CB81\" = 0.0003, \"CB105\" = 0.00003, \"CB118\" = 0.00003,    \"CB126\" = 0.1, \"CB156\" = 0.00003, \"CB157\" = 0.00003, \"CB167\" = 0.00003,    \"CB169\" = 0.03, \"CDD1N\" = 1, \"CDD4X\" = 0.1, \"CDD6P\" = 0.01, \"CDD6X\" = 0.1,    \"CDD9X\" = 0.1, \"CDDO\" = 0.0003, \"CDF2N\" = 0.3, \"CDF2T\" = 0.1, \"CDF4X\" = 0.1,    \"CDF6P\" = 0.01, \"CDF6X\" = 0.1, \"CDF9P\" = 0.01,   \"CDF9X\" = 0.1, \"CDFO\" = 0.00003, \"CDFP2\" = 0.03, \"CDFX1\" = 0.1, \"TCDD\" = 1 )  sediment_timeseries <- create_timeseries(   sediment_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     NAPC1 = list(det = c(\"NAP1M\", \"NAP2M\"), action = \"sum\"),     BD154 = list(det = \"PBB153+BD154\", action = \"replace\"),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     CB156 = list(det = c(\"CB156+172\"), action = \"replace\"),     TEQDFP = list(det = names(info_TEQ), action = \"bespoke\"),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\")   ),   normalise = normalise_sediment_OSPAR,   normalise.control = list(     metals = list(method = \"pivot\", normaliser = \"AL\"),      organics = list(method = \"simple\", normaliser = \"CORG\", value = 2.5),     exclude = expression(ospar_subregion %in% c(\"Iberian Coast\", \"Gulf of Cadiz\"))   ) ) #>  #> Oddities will be written to 'oddities/sediment' with previous oddities backed up to #>  'oddities/sediment_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2016 and 2021  #>    Dropping samples with only auxiliary variables #>    Relabelling matrix SED62 as SED63 and SED500, SED1000, SED2000 as SEDTOT #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>    Missing digestion methods: see digestion_errors.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Large uncertainties: see large_uncertainties.csv #>    Data submitted as BBF, BKF summed to give BBKF #>    Data submitted as BBJF, BKF summed to give BBJKF #>      104 of 104 samples lost due to incomplete submissions #>    Data submitted as NAP1M, NAP2M summed to give NAPC1 #>    Data submitted as  #> CB77, CB81, CB105, CB118, CB126, CB156, CB157, CB167, CB169, CDD1N, CDD4X, CDD6P, CDD6X, CDD9X, CDDO, CDF2N, CDF2T, CDF4X, CDF6P, CDF6X, CDF9P, CDF9X, CDFO, CDFP2, CDFX1, TCDD  #> summed to give TEQDFP #>      17 of 17 samples lost due to incomplete submissions #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Normalising metals to AL using pivot values #>    Inferring missing normaliser digestion from corresponding contaminant digestion #>    Warning: Nss values for CR hard-wired to 5.0 (AL) or 52 (LI) for all digestions #>    Normalising organics to 2.5% CORG #>    Removing sediment data where normaliser is a less than #>    Dropping groups of compounds / stations with no data between 2016 and 2021 sediment_assessment <- run_assessment(   sediment_timeseries,    AC = c(\"BAC\", \"EAC\", \"EQS\", \"ERL\", \"FEQG\"),   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while!  check_assessment(water_assessment) #> All assessment models have converged write_summary_table(   sediment_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\", \"PAH_parent\", \"PAH_alkylated\",         \"PBDEs\", \"Organobromines\", \"Chlorobiphenyls\", \"Dioxins\",        \"Organochlorines\"     ),     labels = c(       \"Metals\", \"Organotins\", \"PAH parent compounds\", \"PAH alkylated compounds\",        \"Polybrominated diphenyl ethers\", \"Organobromines (other)\",        \"Polychlorinated biphenyls\", \"Dioxins\", \"Organochlorines (other)\"     )   ),   classColour = list(     below = c(       \"BAC\" = \"blue\",        \"ERL\" = \"green\",        \"EAC\" = \"green\",        \"EQS\" = \"green\",        \"FEQG\" = \"green\"     ),     above = c(       \"BAC\" = \"orange\",        \"ERL\" = \"red\",        \"EAC\" = \"red\",        \"EQS\" = \"red\",        \"FEQG\" = \"red\"     ),     none = \"black\"   ),   collapse_AC = list(BAC = \"BAC\", EAC = c(\"EAC\", \"ERL\", \"EQS\", \"FEQG\")),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_OSPAR.html","id":"biota","dir":"Articles","previous_headings":"","what":"Biota","title":"HELCOM","text":"finally, let’s take look biota_summary.csv file:","code":"biota_data <- read_data(   compartment = \"biota\",   purpose = \"OSPAR\",   contaminants = \"biota.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),    extraction = \"2023/08/23\" ) #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path thresholds_biota.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/thresholds_biota.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/stations.txt' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/biota.txt' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CF or EF  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 10047 records by coordinates #>  - matching 66378 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 75965 of 76425 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2022 biota_data <- tidy_data(biota_data) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Dropping 11 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - species missing #>  #> Dropping 460 records from data that have no valid station code #>  #> Dropping 13427 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data biota_timeseries <- create_timeseries(   biota_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     NAPC1 = list(det = c(\"NAP1M\", \"NAP2M\"), action = \"sum\"),     BD154 = list(det = \"PBB153+BD154\", action = \"replace\"),     SBDE6 = list(       det = c(\"BDE28\", \"BDE47\", \"BDE99\", \"BD100\", \"BD153\", \"BD154\"),        action = \"sum\"     ),     HBCD = list(det = c(\"HBCDA\", \"HBCDB\", \"HBCDG\"), action = \"sum\"),     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     PFHXS = list(det = c(\"N-PFHXS\", \"BR-PFHXS\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     CB156 = list(det = c(\"CB156+172\"), action = \"replace\"),     SCB6 = list(       det = c(\"CB28\", \"CB52\", \"CB101\", \"CB138\", \"CB153\", \"CB180\"),        action = \"sum\"     ),     SCB7 = list(       det = c(\"CB28\", \"CB52\", \"CB101\", \"CB118\", \"CB138\", \"CB153\", \"CB180\"),        action = \"sum\"     ),     TEQDFP = list(det = names(info_TEQ), action = \"bespoke\"),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\"),     HCH = list(det = c(\"HCHA\", \"HCHB\", \"HCHG\"), action = \"sum\"),     \"LIPIDWT%\" = list(det = c(\"EXLIP%\", \"FATWT%\"), action = \"bespoke\")   ),    get_basis = get_basis_biota_OSPAR ) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2017 and 2022  #>    Dropping samples with only auxiliary variables #>    Unexpected or missing values for 'sex': see sex_queries.csv #>    Unexpected or missing values for 'matrix': see matrix_queries.csv #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Bile metabolite units changed from 'ng/g' to 'ng/ml' and from 'ug/kg' to 'ug/l' #>    Unexpected or missing values for 'unit': see unit_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Unrecognised censoring values: deleted data in 'censoring_codes_unrecognised.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Non-positive uncertainties: see non_positive_uncertainties.csv #>    Large uncertainties: see large_uncertainties.csv #>    Data submitted as CHRTR relabelled as CHR  #>    Data submitted as BBF, BKF summed to give BBKF #>      48 of 68 samples lost due to incomplete submissions #>    Data submitted as BBJF, BKF summed to give BBJKF #>      23 of 60 samples lost due to incomplete submissions #>    Data submitted as BBJKF relabelled as BBKF  #>    Data submitted as NAP1M, NAP2M summed to give NAPC1 #>      1 of 1 samples lost due to incomplete submissions #>    Data submitted as BDE28, BDE47, BDE99, BD100, BD153, BD154 summed to give  #> SBDE6 #>      18 of 42 samples lost due to incomplete submissions #>    Data submitted as HBCDA, HBCDB, HBCDG summed to give HBCD #>      2 of 11 samples lost due to incomplete submissions #>    Data submitted as CB28, CB52, CB101, CB138, CB153, CB180 summed to give SCB6 #>      14 of 60 samples lost due to incomplete submissions #>    Data submitted as CB28, CB52, CB101, CB118, CB138, CB153, CB180  #> summed to give SCB7 #>      14 of 60 samples lost due to incomplete submissions #>    Data submitted as  #> CB77, CB81, CB105, CB118, CB126, CB156, CB157, CB167, CB169, CDD1N, CDD4X, CDD6P, CDD6X, CDD9X, CDDO, CDF2N, CDF2T, CDF4X, CDF6P, CDF6X, CDF9P, CDF9X, CDFO, CDFP2, CDFX1, TCDD  #> summed to give TEQDFP #>      76 of 76 samples lost due to incomplete submissions #>    Data submitted as HCHA, HCHB, HCHG summed to give HCH #>      33 of 119 samples lost due to incomplete submissions #>    Data submitted as EXLIP% or FATWT% relabelled as LIPIDWT%  #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Losing 201 out of 3183 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Dropping bivalve and gastropod contaminant data collected during the #>     spawning season, which is taken to be the following months: #>     April, May, June, July  #>    Dropping groups of compounds / stations with no data between 2017 and 2022 wk_metals <-    c(\"AG\", \"AS\", \"CD\", \"CO\", \"CR\", \"CU\", \"HG\", \"NI\", \"PB\", \"SE\", \"SN\", \"ZN\")  biota_assessment <- run_assessment(   biota_timeseries,    subset = determinand %in% wk_metals,   AC = c(\"BAC\", \"EAC\", \"EQS\", \"HQS\"),   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! biota_assessment <- update_assessment(   biota_assessment,    subset = !determinand %in% wk_metals,   parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(biota_assessment) #> All assessment models have converged write_summary_table(   biota_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\",       \"PAH_parent\", \"PAH_alkylated\", \"Metabolites\",       \"PBDEs\", \"Organobromines\",       \"Organofluorines\",       \"Chlorobiphenyls\", \"Dioxins\", \"Organochlorines\",       \"Effects\"     ),     labels = c(       \"Metals\", \"Organotins\",       \"PAH parent compounds\", \"PAH alkylated compounds\", \"PAH metabolites\",       \"Polybrominated diphenyl ethers\", \"Organobromines (other)\",       \"Organofluorines\",       \"Polychlorinated biphenyls\", \"Dioxins\", \"Organochlorines (other)\",       \"Biological effects (other)\"     )   ),   classColour = list(     below = c(       \"BAC\" = \"blue\",        \"EAC\" = \"green\",        \"EQS\" = \"green\",        \"HQS\" = \"green\"     ),     above = c(       \"BAC\" = \"orange\",        \"EAC\" = \"red\",        \"EQS\" = \"red\",        \"HQS\" = \"red\"     ),     none = \"black\"   ),   collapse_AC = list(BAC = \"BAC\", EAC = c(\"EAC\", \"EQS\"), HQS = \"HQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"read-data","dir":"Articles","previous_headings":"","what":"Read data","title":"External data","text":"Mercury data supporting variables station dictionary","code":"biota_data <- read_data(   compartment = \"biota\",   purpose = \"AMAP\",   contaminants = \"AMAP_external_data_new_data_only_CAN_MarineMammals.csv\",   stations = \"AMAP_external_new_stations_only.csv\",   data_dir = file.path(working.directory, \"data\", \"example_external_data\"),   data_format = \"external\",   info_dir = file.path(working.directory, \"information\", \"AMAP_2022\"),   control = list(region = list(id = \"AMAP_region\")) ) #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/AMAP_2022/determinand.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/AMAP_2022/species.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> [1] \"found in path thresholds_biota.csv /Users/stuart/git/HARSAT/information/AMAP_2022/thresholds_biota.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/AMAP_2022\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_external_data/AMAP_external_new_stations_only.csv' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_external_data/AMAP_external_data_new_data_only_CAN_MarineMammals.csv' #>  #> Argument max_year taken to be the maximum year in the data: 2017"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"prepare-data-for-next-stage","dir":"Articles","previous_headings":"","what":"Prepare data for next stage","title":"External data","text":"Get correct variables streamline data files","code":"biota_data <- tidy_data(biota_data) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Dropping 42 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"construct-timeseries","dir":"Articles","previous_headings":"","what":"Construct timeseries","title":"External data","text":"timeseries, use basis reported often data","code":"biota_timeseries <- create_timeseries(   biota_data,   get_basis = get_basis_most_common ) #>  #> Oddities will be written to 'oddities/biota' with previous oddities backed up to #>  'oddities/biota_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2012 and 2017 #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Unexpected or missing values for 'value': see value_queries.csv #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Dropping groups of compounds / stations with no data between 2012 and 2017"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"assessment","dir":"Articles","previous_headings":"","what":"Assessment","title":"External data","text":"Main runs Use code takes long time run","code":"biota_assessment <- run_assessment(   biota_timeseries,    AC = c(\"NRC\", \"LRC\", \"MRC\", \"HRC\") ) #>  #> assessing series:  station_code A1; determinand HG; species Phoca hispida; matrix LI; subseries adult; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A1; determinand HG; species Phoca hispida; matrix LI; subseries juvenile; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A1; determinand HG; species Phoca hispida; matrix MU; subseries adult; basis W; unit ug/kg  #>  #> assessing series:  station_code A1; determinand HG; species Phoca hispida; matrix MU; subseries juvenile; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A20; determinand HG; species Phoca hispida; matrix LI; subseries adult; basis W; unit ug/kg  #>  #> assessing series:  station_code A20; determinand HG; species Phoca hispida; matrix LI; subseries juvenile; basis W; unit ug/kg  #>  #> assessing series:  station_code A20; determinand HG; species Phoca hispida; matrix MU; subseries adult; basis W; unit ug/kg  #>  #> assessing series:  station_code A20; determinand HG; species Phoca hispida; matrix MU; subseries juvenile; basis W; unit ug/kg  #>  #> assessing series:  station_code A3; determinand HG; species Delphinapterus leucas; matrix LI; subseries large; basis W; unit ug/kg  #>  #> assessing series:  station_code A3; determinand HG; species Delphinapterus leucas; matrix LI; subseries small; basis W; unit ug/kg  #>  #> assessing series:  station_code A3; determinand HG; species Delphinapterus leucas; matrix MU; subseries large; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A3; determinand HG; species Delphinapterus leucas; matrix MU; subseries small; basis W; unit ug/kg  #>  #> assessing series:  station_code A3; determinand HG; species Ursus maritimus; matrix LI; subseries adult_female; basis D; unit ug/kg #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>  #> assessing series:  station_code A3; determinand HG; species Ursus maritimus; matrix LI; subseries adult_male; basis D; unit ug/kg #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>  #> assessing series:  station_code A3; determinand HG; species Ursus maritimus; matrix LI; subseries juvenile; basis D; unit ug/kg #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A6; determinand HG; species Phoca hispida; matrix LI; subseries adult; basis W; unit ug/kg  #>  #> assessing series:  station_code A6; determinand HG; species Phoca hispida; matrix LI; subseries juvenile; basis W; unit ug/kg  #>  #> assessing series:  station_code A6; determinand HG; species Phoca hispida; matrix MU; subseries adult; basis W; unit ug/kg  #>  #> assessing series:  station_code A6; determinand HG; species Phoca hispida; matrix MU; subseries juvenile; basis W; unit ug/kg  #>  #> assessing series:  station_code A6; determinand HG; species Ursus maritimus; matrix LI; subseries adult_female; basis D; unit ug/kg #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A6; determinand HG; species Ursus maritimus; matrix LI; subseries adult_male; basis D; unit ug/kg #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>  #> assessing series:  station_code A6; determinand HG; species Ursus maritimus; matrix LI; subseries juvenile; basis D; unit ug/kg #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>    Losing 1 out of 1 records in basis conversion due to missing, censored #>    or zero drywt or lipidwt values. #>  #> assessing series:  station_code A8; determinand HG; species Phoca hispida; matrix LI; subseries adult; basis W; unit ug/kg  #>  #> assessing series:  station_code A8; determinand HG; species Phoca hispida; matrix LI; subseries juvenile; basis W; unit ug/kg  #>  #> assessing series:  station_code A8; determinand HG; species Phoca hispida; matrix MU; subseries adult; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A8; determinand HG; species Phoca hispida; matrix MU; subseries juvenile; basis W; unit ug/kg  #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix EP; subseries large; basis W; unit ug/kg  #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix EP; subseries small; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix KI; subseries large; basis W; unit ug/kg  #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix KI; subseries small; basis W; unit ug/kg  #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix LI; subseries large; basis W; unit ug/kg  #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix LI; subseries small; basis W; unit ug/kg #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix MU; subseries large; basis W; unit ug/kg  #>  #> assessing series:  station_code A9; determinand HG; species Delphinapterus leucas; matrix MU; subseries small; basis W; unit ug/kg biota_assessment <- run_assessment(   biota_timeseries,   AC = c(\"NRC\", \"LRC\", \"MRC\", \"HRC\")   parallel = TRUE )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"check-convergence","dir":"Articles","previous_headings":"Assessment","what":"Check convergence","title":"External data","text":"","code":"check_assessment(biota_assessment) #> All assessment models have converged"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"summary-files","dir":"Articles","previous_headings":"","what":"Summary files","title":"External data","text":"writes summary data file output/example_external_data.","code":"summary.dir <- file.path(working.directory, \"output\", \"example_external_data\")  if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) }   write_summary_table(   biota_assessment,   output_dir = summary.dir,     classColour = list(     below = c(\"NRC\" = \"blue\", \"LRC\" = \"green\", \"MRC\" = \"orange\", \"HRC\" = \"darkorange\"),     above = c(\"NRC\" = \"red\", \"LRC\" = \"red\", \"MRC\" = \"red\", \"HRC\" = \"red\"),     none = \"black\"   ) )"},{"path":"http://osparcomm.github.io/HARSAT/articles/example_external_data.html","id":"graphics-output","dir":"Articles","previous_headings":"","what":"Graphics output","title":"External data","text":"Plots assessment either data (file_type = “data”) annual index (file_type = “index”) (default) Outputs can png pdf Can subset assessment based variables either timeSeries stations components object: commonly determinand, matrix, species, station_code station_name; can also use series identifier row.names(timeSeries) subset NULL (default), timeseries plotted (can take time) Graphics plots written files output/graphics.","code":"graphics.dir <- file.path(working.directory, \"output\", \"graphics\")  if (!dir.exists(graphics.dir)) {   dir.create(graphics.dir, recursive = TRUE) }   plot_assessment(   biota_assessment,   subset = species %in% \"Phoca hispida\",   output_dir = graphics.dir,    file_type = \"data\",   file_format = \"png\" ) plot_assessment(   biota_assessment,   subset = matrix %in% \"LI\",   output_dir = graphics.dir,    file_type = \"index\",   file_format = \"pdf\" ) plot_assessment(   biota_assessment,   subset = station_code %in% \"A1\",   output_dir = graphics.dir,    file_type = \"data\",   file_format = \"pdf\" ) plot_assessment(   biota_assessment,    subset = series == \"A1 HG Phoca hispida LI adult\",   output_dir = graphics.dir,    file_format = \"pdf\" )"},{"path":[]},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"File management","text":"R package harsat designed make easy work files. harsat uses following different kinds file: Data files. stored data directory anywhere file system. actual data (contaminant measurments station dictionary) files. TAB files downloaded ICES webservice (.e. ICES format) CSV files put together (simpler external format). Analysis files. drive actual analysis. Three files particulary important: determinand file, species file (biota assessment) threshold file. normally keep analysis directory somewhere file system. CSV files, smaller won’t change anywhere near often data files. Configuration files. several additional configuration files provide information , example, chemical methods pivot values sediment normalisation. harsat package provides default versions files fine assessments. However, may override defaults putting modified copies files analysis directory described . configuration files also CSV files. Datasets page provides zip files : data assessment files vignette analysis files recent OSPAR, HELCOM AMAP assessments default additional configuration files need assemble reliable set files assessment, data directory analysis directory. support full reproducibility, good practice also put copy configuration files (whether modified ) analysis directory. updates R harsat package may change contents default configuration files. data files analysis files, copied modified configuration files put analysis directory, affected.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"typical-workflow","dir":"Articles","previous_headings":"","what":"Typical workflow","title":"File management","text":"Let’s imagine want run analysis harsat, already installed R package (described Getting started page). Now need data. Typically get ICES webservice, put together data files using simpler external format. now let’s imagine want try OSPAR vignette. can navigate Datasets page, look approprate zip file download OSPAR vignette. download unzip file (can unzip anywhere like, let’s pretend ’re using Windows unzip : C:\\Users\\stuart\\OSPAR_vignette) ’ll see disk contains files follows: means directories follows: Data directory: C:\\Users\\stuart\\OSPAR_vignette\\data Analysis directory: C:\\Users\\stuart\\OSPAR_vignette\\analysis Obviously, can put directories anywhere like file system. can even put removable disk like, network shared drive. can also call directories something else. example, might call data_vignette analysis_vignette distinguish assessments. , now let’s see might use run analysis.","code":"+ C:\\Users\\stuart\\OSPAR_vignette   |   + data   | |   | + test_data.csv   | + station_dictionary.csv   | + quality_assurance.csv   |   + analysis      |     + determinand.csv     + species.csv     + thresholds_biota.csv"},{"path":"http://osparcomm.github.io/HARSAT/articles/file-management.html","id":"reading-your-data-files","dir":"Articles","previous_headings":"","what":"Reading your data files","title":"File management","text":"Virtually work need involves call harsat’s read_data() function. Let’s suppose R working directory (R Project) C:\\Users\\stuart\\OSPAR_vignette\\. call typically look like : default, function looks data analysis files directories called data assessment nested inside working directory. called something else, can use data_dir analysis_dir arguments. example: can also specify absolute path names important things see : Note use file.path() make portable pathnames. course, user can use whatever filename pattern works best . info_path parameter can vector well single string. harsat actually search every directory vector, looking files like determinand.csv. file found local analysis directory, gets read used. , may try directories vector. get end ’ve still found particular file (especially common standard ones like matrix.csv translates common codes) harsat’s built-directory configuration files gets used last resort. file really essential still can’t find , harsat immediately throw error can intervene. harsat , log file actually used, also log “thumbprint” file contents – typically something like string hexadecimal digits. whereever file comes , long contents file . move file different directory don’t edit , thumbprint show. , thumbprints vital tool tracking reproducibility, change contents data changes.","code":"biota_data <- read_data(   compartment = \"biota\",    purpose = \"OSPAR\",                                  contaminants = \"test_data.csv\",    stations = \"station_dictionary.csv\",    data_format = \"ICES\", ) biota_data <- read_data(   compartment = \"biota\",    purpose = \"OSPAR\",                                  contaminants = \"test_data.csv\",    stations = \"station_dictionary.csv\",   data_dir = \"data_vignette\",   data_format = \"ICES\",   analysis_dir = \"analysis_vignette\" ) biota_data <- read_data(   compartment = \"biota\",    purpose = \"OSPAR\",                                  contaminants = \"test_data.csv\",    stations = \"station_dictionary.csv\",   data_dir = file.path(\"C:\", \"Users\", \"stuart\", \"OSPAR_vignette\", \"data\"),   data_format = \"ICES\",   analysis_dir = file.path(\"C:\", \"Users\", \"stuart\", \"somewhere_else\", \"assessment\"), )"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"installing-harsat-from-github","dir":"Articles","previous_headings":"","what":"Installing harsat from GitHub","title":"Getting started","text":"install latest development version, use remotes package: stable version similar: Note: development repository marked private GitHub, need Personal Access Token (PAT) access . Follow instructions create Personal Access Token. ’ll short string, probably beginning ghp_. Put whole string auth_token parameter, install harsat package directly. Note: many functions currently cstm prefix. code originally developed, thought “contaminant time series modelling”, get cstm prefixes. removed near future.","code":"library(remotes) remotes::install_github(\"osparcomm/harsat\", auth_token = 'XXXX') # install.packages(\"devtools\") devtools::install_github(\"osparcomm/HARSAT@main\")"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"loading-the-code","dir":"Articles","previous_headings":"","what":"Loading the code","title":"Getting started","text":"Now, within R, can load library usual way","code":"library(harsat)"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"accessing-files","dir":"Articles","previous_headings":"","what":"Accessing files","title":"Getting started","text":"organize files . start current working directory. ’ll detect using R’s package, write variable working.directory.","code":"library(here) working.directory <- here()"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"reading-in-the-data","dir":"Articles","previous_headings":"","what":"Reading in the data","title":"Getting started","text":"first step read data ’ve got. go arguments. main arguments follows: compartment argument specifies whether ’re dealing biota assessment, sediment assessment, water assessment. purpose means can mirror OSPAR style assessment, HELCOM style assessment, AMAP-style assessment, means can basically tailor idea code sufficiently flexible can lot tailoring suit needs. contaminants data file chemical measurements . stations station file directly related station dictionary get ICES. info_dir directory reference tables – come back . reads three data sets, stage. get point, can look data want , anything else need data proceeding. Essentially files come point unchanged files reading . basically just reading data setting things . point might want whole lot ad hoc corrections data, done OSPAR estimates.","code":"water_data <- read_data(   compartment = \"water\",   purpose = \"OSPAR\",   contaminants = \"water.txt\",   stations = \"stations.txt\",   data_dir = file.path(working.directory, \"data\", \"example_OSPAR\"),   info_dir = file.path(working.directory, \"information\", \"OSPAR_2022\"),   extraction = \"2023/08/23\" ) #> [1] \"found in path determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/determinand.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/species.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in path thresholds_water.csv /Users/stuart/git/HARSAT/information/OSPAR_2022/thresholds_water.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package method_extraction.csv /Users/stuart/git/HARSAT/information/method_extraction.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package pivot_values.csv /Users/stuart/git/HARSAT/information/pivot_values.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package matrix.csv /Users/stuart/git/HARSAT/information/matrix.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> [1] \"found in package imposex.csv /Users/stuart/git/HARSAT/information/imposex.csv /Users/stuart/git/HARSAT/information/OSPAR_2022\" #> Reading station dictionary from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/stations.txt' #>  #> Reading contaminant and effects data from: #>  '/Users/stuart/git/HARSAT/data/example_OSPAR/water.txt' #>  #> Matching data with station dictionary #>  - restricting to stations in these convention areas: OSPAR  #>  - restricting to station with data types CW or EW  #>  - restricting to stations marked for temporal monitoring #>  - restricting to data with program governance: OSPAR, AMAP  #>  - restricting to stations with program governance: OSPAR, AMAP  #>  - matching 2768 records by coordinates #>  - matching 1583 records by station name #>  - grouping stations using station_asmtmimegovernance #>  - 4251 of 4351 records have been matched to a station #>  #> Argument max_year taken to be the maximum year in the data: 2022"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"tidying-the-data","dir":"Articles","previous_headings":"","what":"Tidying the data","title":"Getting started","text":"next step clean data prepare analysis. step tidies data structures ’ve got . filtering get data form want say, OSPAR assessment HELCOM assessment. also streamlines data files. may generate warnings. example, ’re cleaning station dictionary, ’ve may find issues duplicate stations. Similarly, comes cleaning contaminant biological effects data, ’ve may find data stations unrecognized station dictionary. Sometimes ’s fine sometimes ’s . Warnings supported output files allow come look see values affected, can go check detail. point, ’ve still done anything datasets.","code":"water_data <- tidy_data(water_data) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Dropping 411 records from data flagged for deletion. Possible reasons are: #>  - vflag = suspect #>  - upper depth >5.5m #>  - filtration missing #>  - matrix = 'SPM' #>  #> Dropping 94 records from data that have no valid station code #>  #> Dropping 13633 stations that are not associated with any data #>  #> Cleaning station dictionary #>  #> Cleaning contaminant and biological effects data"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"create-a-time-series","dir":"Articles","previous_headings":"","what":"Create a time series","title":"Getting started","text":"Now can group data time series. means identifying data points belong time series, set structure allows us assessments. various different ways can specify determinants actually want assess. case, determinands parameter specifies CHR, BBKF, PFOS, CB138, HCEPX, HCH. various arguments allow control just data manipulated.","code":"water_timeseries <- create_timeseries(   water_data,   determinands.control = list(     CHR = list(det = \"CHRTR\", action = \"replace\"),     BBKF = list(det = c(\"BBF\", \"BKF\", \"BBJF\", \"BBJKF\"), action = \"bespoke\"),     PFOS = list(det = c(\"N-PFOS\", \"BR-PFOS\"), action = \"sum\"),     CB138 = list(det = c(\"CB138+163\"), action = \"replace\"),     HCEPX = list(det = c(\"HCEPC\", \"HCEPT\"), action = \"sum\"),      HCH = list(det = c(\"HCHA\", \"HCHB\", \"HCHG\"), action = \"sum\")   ) ) #>  #> Oddities will be written to 'oddities/water' with previous oddities backed up to #>  'oddities/water_backup' #>  #> Cleaning data #>    Dropping stations with no data between 2017 and 2022 #>    Unexpected or missing values for 'basis': see basis_queries.csv #>    Replicate measurements, only first retained: see replicate_measurements.csv #>    Non-positive detection limits: see non_positive_det_limits.csv #>    Limit of quantification less than limit of detection: see limits_inconsistent.csv #>    Censoring codes D and Q inconsistent with respective limits: see censoring_codes_inconsistent.csv #>    Detection limit higher than data: see detection_limit_high.csv #>    Data submitted as CHRTR relabelled as CHR  #>    Data submitted as BBF, BKF summed to give BBKF #>      1 of 71 samples lost due to incomplete submissions #>    Data submitted as BBJF, BKF summed to give BBJKF #>      99 of 99 samples lost due to incomplete submissions #>    Data submitted as HCEPC, HCEPT summed to give HCEPX #>    Data submitted as HCHA, HCHB, HCHG summed to give HCH #>      190 of 234 samples lost due to incomplete submissions #>  #> Creating time series data #>    Converting data to appropriate basis for statistical analysis #>    Dropping groups of compounds / stations with no data between 2017 and 2022"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"assessment","dir":"Articles","previous_headings":"","what":"Assessment","title":"Getting started","text":"next next stage assessment. ’ve created time series object, pass call. say thresholds ’re going use assessment options can put . just run. can see tells time series ’re actually assessing progresses. gives idea many cups tea can drink ’s finished. various little warnings: ones nothing worry . can check everything converged.","code":"water_assessment <- run_assessment(   water_timeseries,    AC = \"EQS\",    parallel = TRUE ) #> Setting up clusters: be patient, it can take a while! check_assessment(water_assessment) #> All assessment models have converged"},{"path":"http://osparcomm.github.io/HARSAT/articles/harsat.html","id":"reporting","dir":"Articles","previous_headings":"","what":"Reporting","title":"Getting started","text":"can get summary table results. want direcrory can put . harsat won’t create directory ’s nothing , let’s make new directory, ./output/tutorial, put full path summary.dir, can tell harsat write . can generate summary proper. summary file familiar involved OSPAR HELCOM assessments. summary files information time series, time series represents (first set columns), followed statistical results, p values, summary values number years data set, starts finishes. towards end, ’ve got comparisons various different threshold values.","code":"summary.dir <- file.path(working.directory, \"output\", \"tutorial\")  if (!dir.exists(summary.dir)) {   dir.create(summary.dir, recursive = TRUE) } write_summary_table(   water_assessment,   determinandGroups = list(     levels = c(       \"Metals\", \"Organotins\", \"PAH_parent\",  \"Organofluorines\",        \"Chlorobiphenyls\", \"Organochlorines\", \"Pesticides\"     ),       labels = c(       \"Metals\", \"Organotins\", \"PAH parent compounds\", \"Organofluorines\",        \"Polychlorinated biphenyls\", \"Organochlorines (other)\", \"Pesticides\"     )   ),   classColour = list(     below = c(\"EQS\" = \"green\"),      above = c(\"EQS\" = \"red\"),      none = \"black\"   ),   collapse_AC = list(EAC = \"EQS\"),   output_dir = summary.dir )"},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"species-file-format","dir":"Articles","previous_headings":"","what":"Species file format","title":"Reference table file formats","text":"species file species.csv information files directory. can many dry lipid weight columns tissue types. example, blood (BL) data, can columns BL_drywt BL_lipidwt. (Remember difference BL (blood) ER (erythrocytes - red blood cells vertebrates).","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"determinand-file-format","dir":"Articles","previous_headings":"","what":"Determinand file format","title":"Reference table file formats","text":"determinand file determinand.csv information files directory.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"thresholds-file-format","dir":"Articles","previous_headings":"","what":"Thresholds file format","title":"Reference table file formats","text":"thresholds files (usually several, one compartment) might vary considerably compartments. However, principle : set values given determinand. case biota, ’s linked particular species species group. case sediment, linked, typically, region. Water simplest, let’s look first:","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"water-thresholds-files","dir":"Articles","previous_headings":"Thresholds file format","what":"Water thresholds files","title":"Reference table file formats","text":"water thresholds file thresholds_water.csv information files directory. illustration, suppose just one threshold, EQS. thresholds file following four columns: another threshold, example, BAC, add two columns called BAC_basis BAC. can many thresholds want. , particular determinand, (set ) threshold value(s) applied filtered unfiltered time series, set filtration filtered~unfiltered.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"sediment-thresholds-files","dir":"Articles","previous_headings":"Thresholds file format","what":"Sediment thresholds files","title":"Reference table file formats","text":"sediment thresholds file thresholds_sediment.csv information files directory. illustratoin, suppose two thresholds, BAC EAC. thresholds file following five columns: , can many thresholds want. example, OSPAR assessments typically use ERL, EQS FEQG addition BAC EAC. can also extra columns match columns station dictionary. Typically, used apply different threshold values different regions. example, OSPAR thresholds file column ospar_subregion, allows one set threshold values applied Iberian Coast Gulf Cadiz another set rest OSPAR area.","code":""},{"path":"http://osparcomm.github.io/HARSAT/articles/reference-file-formats.html","id":"biota-thresholds-files","dir":"Articles","previous_headings":"Thresholds file format","what":"Biota thresholds files","title":"Reference table file formats","text":"biota thresholds file thresholds_biota.csv information files directory. Note none species_group, species_subgroup, species mandatory, row, needs enough match species lists. unspecified.","code":""},{"path":"http://osparcomm.github.io/HARSAT/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Rob Fryer. Author. Leszek Kaliciak. Author. Scottish Government. Funder, author. AmbieSense Ltd. Copyright holder, maintainer, author. Helsinki Commission (HELCOM). Copyright holder, funder, contributor. Arctic Monitoring Assessment Programme (AMAP). Copyright holder, funder, contributor. OSPAR Commission (OSPAR). Copyright holder, funder, contributor. International Council Exploration Sea (ICES). Copyright holder, author.","code":""},{"path":"http://osparcomm.github.io/HARSAT/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Fryer R, Kaliciak L, Scottish Government, AmbieSense Ltd, International Council Exploration Sea (ICES) (2023). harsat: Harmonized Regional Seas Assessment Tool. https://github.com/osparcomm/HARSAT, https://osparcomm.github.io/HARSAT/.","code":"@Manual{,   title = {harsat: Harmonized Regional Seas Assessment Tool},   author = {Rob Fryer and Leszek Kaliciak and {Scottish Government} and {AmbieSense Ltd} and {International Council for the Exploration of the Sea (ICES)}},   year = {2023},   note = {https://github.com/osparcomm/HARSAT, https://osparcomm.github.io/HARSAT/}, }"},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"harsat","dir":"","previous_headings":"","what":"Harmonized Regional Seas Assessment Tool","title":"Harmonized Regional Seas Assessment Tool","text":"Requirements: R programming language version 4.2.1 RStudio Integrated Development Environment version 2022.07.1 Build 554 Additional R packages come standard RStudio installation. installed, either using RStudio GUI command install.packages e.g. install.packages(\"lme4\"): tidyverse version 2.0.0 dplyr lubridate readr stringr tibble tidyr sf lme4 mgcv mvtnorm numDeriv optimx pbapply parallel flexsurv following R packages need installed run HELCOM example: rmarkdown htmlwidgets File -> New Project RStudio create new project. File -> New Project -> Version Control -> GIT -> https://github.com/osparcomm/HARSAT clone repository. example datasets can run basic directory structure. following directories manually created: data R information output example scripts currently use subfolders within data output folders example_external_data example_HELCOM example_OSPAR example_external_data example_HELCOM example_OSPAR data output can go anywhere. Running code create directory called oddities data might comply reporting requirements posted (warnings errors).","code":""},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Harmonized Regional Seas Assessment Tool","text":"can install development version harsat GitHub : stable version similar:","code":"# install.packages(\"devtools\") devtools::install_github(\"osparcomm/HARSAT\") # install.packages(\"devtools\") devtools::install_github(\"osparcomm/HARSAT@main\")"},{"path":"http://osparcomm.github.io/HARSAT/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Harmonized Regional Seas Assessment Tool","text":"basic example shows solve common problem: following figures bit date: Figure 1. Required Project File Structure OSPAR dataset. Figure 2. Required Project File Structure HELCOM dataset. R functions GitHub code repository moved R, csv files reference tables go ‘information’. recommended R files examples, data output directories (project) directory e.g. HARSAT HARSAT HARSAT2022.r although one can adjust path argument ctsm_read_data ctsm.summary.table. ‘functions’ ‘information’ directories can anywhere (although recommended got directory), function_path variable (top OSPAR 2022.r) pointing former. Data required OSPAR: Figure 3. OSPAR data. Data required HELCOM: Figure 4. HELCOM data. HELCOM dataset requires additional R files work, different reference tables ‘assessment criteria biota.csv’ ‘assessment criteria sediment.csv’. HELCOM also requires different information functions repository different version HELCOM specific functions (‘information functions v2_68.r’). Figure 5. Additional R files required HELCOM dataset.","code":"library(harsat) ## basic example code"},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":null,"dir":"Reference","previous_headings":"","what":"Add stations to contaminant data from an ICES extraction — add_stations","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"Adds station name station code contaminant data ICES extraction. done either matching station names submitted data station dictionary, matching sample coordinates station dictionary, combination .","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"","code":"add_stations(data, stations, info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"data data frame contaminant data ICES extraction stations data frame ICES station dictionary info HARSAT information list must contain elements purpose, compartment, add_stations. latter list control parameters supplied control_default control_modify control station matching achieved. See details. compartment string: \"biota\", \"sediment\" \"water\"","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"data frame containing contaminant data augmented variables containing station code station name","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/add_stations.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add stations to contaminant data from an ICES extraction — add_stations","text":"info$add_stations list control parameters modify station matching process: method: string specifying whether stations matched \"name\", \"coordinates\", \"\". info$purpose \"custom\", method restricted either \"name\" (default) \"coordinates\". info$purpose \"OSPAR\", \"HELCOM\" \"AMAP\", method set \"\" default stations matched name coordinates according rules specified OSPAR, HELCOM AMAP data providers. Specifically, stations matched name Denmark, France (biota water - years; sediment 2009 onwards), Ireland, Norway, Portugal, Spain (2005 onwards), Sweden, Netherlands (2007 onwards), United Kingdom. stations matched coordinates. area: vector strings containing one \"OSPAR\", \"HELCOM\" \"AMAP\"; restricts stations corresponding convention area(s); NULL matches stations station dictionary datatype: logical specifying whether stations restricted appropriate datatype. TRUE, contaminant measurement biota (example) matched stations station_datatype containing string \"CF\". Similarly, biological effect measurement biota matched stations station_datatype containing string \"EF\" temporal: logical TRUE indicating stations restricted station_purpm containing string \"T\" governance_type: string: \"none\", \"data\", \"stations\" \"\". \"none\" means data station governance ignored. \"data\" means matching restricted data governance station governance; example governance_id == c(\"OSPAR\", \"AMAP\"), data matched station one is_ospar_monitoring is_amap_monitoring TRUE, stations considered regardless station governance. \"stations\" mean matching restricted station governance data governance; example governance_id == c(\"OSPAR\", \"AMAP\"), stations restricted station_programgovernance contains either \"OSPAR\" \"AMAP\", data considered regardless data governance. uses data station governance. governance_id contains single value, matching strict. However, governance_id contains multiple values, matching complicated. example, governance_id == c(\"OSPAR\", \"AMAP\"), measurements is_ospar_monitoring == TRUE \"is_amap_monitoring == FALSE\" matched stations station_programgovernance contains \"OSPAR\"; measurements is_ospar_monitoring == FALSEandis_amap_monitoring == TRUEare matched stations wherestation_programgovernancecontains\"AMAP\"; measurements is_ospar_monitoring == TRUEandis_amap_monitoring == TRUEare matched stations wherestation_programgovernancecontains either\"OSPAR\"\"AMAP\"`. governance_id: vector strings containing one \"OSPAR\", \"HELCOM\" \"AMAP\". grouping: logical TRUE indicating stations grouped meta-stations specified station_asmtmimeparent station dictionary. Defaults FALSE apart info$purpose == \"OSPAR\". check_coordinates: logical TRUE indicating , stations matched name, sample coordinates must also within station geometry. implemented yet, defaults ot FALSE.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether the assessments have converged — check_assessment","title":"Check whether the assessments have converged — check_assessment","text":"Checks whether assessments HARSAT assessment object converged. Currently detailed checks models normal lognormal errors (chemical timeseries biological effects). checks whether: fixed effect estimates away bounds random effect estimates lower upper bounds; can course equal zero standard errors present model predictions fixed effects estimates standard errors fixed effects estimates realistic (small standard errors indicate problems numerical differencing used compute standard errors)","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether the assessments have converged — check_assessment","text":"","code":"check_assessment(assessment_ob, save_result = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether the assessments have converged — check_assessment","text":"assessment_ob HARSAT assessment object resulting call ctsm_assessment save_result Saves identifiers timeseries converged; defaults FALSE. assessment done stages, output also identifies timeseries yet assessed","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_assessment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether the assessments have converged — check_assessment","text":"list two character vectors: not_converged identifies timeseries converged not_assessed identifies timeseries assessed","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":null,"dir":"Reference","previous_headings":"","what":"Checks convergence of an assessment model — check_convergence_lmm","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"Utilty function use within assess_lmm. Checks whether model converged. Currently checks assessments normal (lognormal) errors, considers whether: fixed effect estimates away bounds random effect estimates lower upper bounds; can course equal zero standard errors present model predictions fixed effects estimates standard errors fixed effects estimates unrealistically small; tolerance chosen much smaller seen typical OSPAR assessments converged Model fits based distributions assumed converged. checking needed future","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"","code":"check_convergence_lmm(assessment, coeff_se_tol = 0.001)"},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"assessment assessment assess_lmm coeff_se_tol tolerance checking whether standard errors fixed effects estimates unrealistically small; defaults 0.001","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/check_convergence_lmm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Checks convergence of an assessment model — check_convergence_lmm","text":"integer: 0 indicates convergence, 1 indicates issue","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":null,"dir":"Reference","previous_headings":"","what":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"utility function expand simplified OSPAR sediment threshold table (much easier user edit) form required harsat","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"","code":"convert_reftable(input_file, output_file, export = TRUE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"input_file input reference file output_file expanded reference file export boolean flag, FALSE, data returned rather written output_file","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_reftable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add OSPAR_subregion to simplified sediment threshold reference table — convert_reftable","text":"export FALSE (default), returns expanded data","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_units.html","id":null,"dir":"Reference","previous_headings":"","what":"Converts units, e.g., from mg/kg to ug/kg — convert_units","title":"Converts units, e.g., from mg/kg to ug/kg — convert_units","text":"Can accept non-standard units (e.g. biological effects) provided rows identical (case attempt made convert.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_units.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Converts units, e.g., from mg/kg to ug/kg — convert_units","text":"","code":"convert_units(conc, from, to)"},{"path":"http://osparcomm.github.io/HARSAT/reference/convert_units.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Converts units, e.g., from mg/kg to ug/kg — convert_units","text":"conc value convert current units required units","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/create_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a time series — create_timeseries","title":"Create a time series — create_timeseries","text":"Cleans data turns time series structures ready assessment","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/create_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a time series — create_timeseries","text":"","code":"create_timeseries(   ctsm.obj,   determinands = ctsm_get_determinands(ctsm.obj$info),   determinands.control = NULL,   oddity_path = \"oddities\",   return_early = FALSE,   print_code_warnings = FALSE,   get_basis = get_basis_default,   normalise = FALSE,   normalise.control = list() )"},{"path":"http://osparcomm.github.io/HARSAT/reference/cstm.VDS.environment.html","id":null,"dir":"Reference","previous_headings":"","what":"Detects the environment from the call — cstm.VDS.environment","title":"Detects the environment from the call — cstm.VDS.environment","text":"utility function detects package environment. imported harsat package, returns package environment. Otherwise, returns global environment. can safely export functions result , example, setting cluster child processes parallel computation.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/cstm.VDS.environment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detects the environment from the call — cstm.VDS.environment","text":"","code":"cstm.VDS.environment()"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.cl.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.cl — ctsm.VDS.cl","title":"ctsm.VDS.cl — ctsm.VDS.cl","text":"ctsm.VDS.cl","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.cl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.cl — ctsm.VDS.cl","text":"","code":"ctsm.VDS.cl(fit, nsim = 1000)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.index.opt.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","title":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","text":"ctsm.VDS.index.opt","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.index.opt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.index.opt — ctsm.VDS.index.opt","text":"","code":"ctsm.VDS.index.opt(data, theta, refLevel, calc.vcov = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.loglik.calc.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","title":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","text":"ctsm.VDS.loglik.calc","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.loglik.calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","text":"","code":"ctsm.VDS.loglik.calc(   theta,   data,   index.theta,   minus.twice = FALSE,   cumulate = FALSE )"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.loglik.calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ctsm.VDS.loglik.calc — ctsm.VDS.loglik.calc","text":"cumulate boolean, whether use cumulative probabilities","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.p.calc.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","title":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","text":"ctsm.VDS.p.calc","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.p.calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","text":"","code":"ctsm.VDS.p.calc(theta, cumulate = FALSE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.p.calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ctsm.VDS.p.calc — ctsm.VDS.p.calc","text":"cumulate boolean, whether use cumulative probabilities","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.varlist.html","id":null,"dir":"Reference","previous_headings":"","what":"ctsm.VDS.varlist — ctsm.VDS.varlist","title":"ctsm.VDS.varlist — ctsm.VDS.varlist","text":"list names functions values necessary export cluster prcesses parallel computation","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.varlist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ctsm.VDS.varlist — ctsm.VDS.varlist","text":"","code":"ctsm.VDS.varlist"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm.VDS.varlist.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ctsm.VDS.varlist — ctsm.VDS.varlist","text":"object class character length 4.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_TBT_convert.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert tin concentrations — ctsm_TBT_convert","title":"Convert tin concentrations — ctsm_TBT_convert","text":"Convert tin concentrations cation concentrations. Also change_unit moves units tin units conventional units","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_TBT_convert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert tin concentrations — ctsm_TBT_convert","text":"","code":"ctsm_TBT_convert(   data,   subset,   action,   from = c(\"tin\", \"cation\"),   convert_var = c(\"value\", \"limit_detection\", \"limit_quantification\", \"uncertainty\") )"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the determinands to be assessed — ctsm_get_determinands","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"Gets determinands assessed determinand reference table.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"","code":"ctsm_get_determinands(info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"info list least following two components: compartment One \"biota\", \"sediment\" \"water\" determinand data frame holding determinand reference table","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"character string containing determinands assessed. function fail error message determinands.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_determinands.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get the determinands to be assessed — ctsm_get_determinands","text":"determinands taken column biota_assess, sediment_assess water_assess determinand reference table (compartment given info$compartment). TRUE values determinands assessed.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Gets data from the standard reference tables — ctsm_get_info","title":"Gets data from the standard reference tables — ctsm_get_info","text":"Gets data standard reference tables","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/ctsm_get_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gets data from the standard reference tables — ctsm_get_info","text":"","code":"ctsm_get_info(   ref_table,   input,   output,   compartment = NULL,   na_action = c(\"fail\", \"input_ok\", \"output_ok\", \"ok\"),   info_type = NULL,   sep = \".\" )"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC.html","id":null,"dir":"Reference","previous_headings":"","what":"Access function map — get_AC","title":"Access function map — get_AC","text":"Access function map","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access function map — get_AC","text":"","code":"get_AC"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_AC.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Access function map — get_AC","text":"object class list length 3.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_biota_OSPAR.html","id":null,"dir":"Reference","previous_headings":"","what":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","title":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","text":"Gets OSPAR biota target basis","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_basis_biota_OSPAR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gets the OSPAR biota target basis — get_basis_biota_OSPAR","text":"","code":"get_basis_biota_OSPAR(data, info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_station_code.html","id":null,"dir":"Reference","previous_headings":"","what":"Get station code from station name — get_station_code","title":"Get station code from station name — get_station_code","text":"Gets station code corresponding station name country station dictionary. works one country time","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_station_code.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get station code from station name — get_station_code","text":"","code":"get_station_code(station_name, country, stations)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts timeSeries — get_timeseries","title":"Extracts timeSeries — get_timeseries","text":"Gets timeSeries component harsat object, optionally added extra information station","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts timeSeries — get_timeseries","text":"","code":"get_timeseries(harsat_obj, add = TRUE)"},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts timeSeries — get_timeseries","text":"harsat_obj harsat object following call create_timeseries.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/get_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts timeSeries — get_timeseries","text":"data.frame containing timeSeries component (optionally) extra information station.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/harsat-package.html","id":null,"dir":"Reference","previous_headings":"","what":"harsat: Harmonized Regional Seas Assessment Tool — harsat-package","title":"harsat: Harmonized Regional Seas Assessment Tool — harsat-package","text":"Assessment concentrations hazardous substances biological effects marine environment. code supports periodic international assessments OSPAR, HELCOM AMAP also assessments individual users. Includes tools pre-processing data, statistical trend analysis comparison threshold values, post-processing archiving reporting.","code":""},{"path":[]},{"path":"http://osparcomm.github.io/HARSAT/reference/harsat-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"harsat: Harmonized Regional Seas Assessment Tool — harsat-package","text":"Maintainer: AmbieSense Ltd info@ambiesense.com [copyright holder] Authors: Rob Fryer rob.fryer@gov.scot Leszek Kaliciak leszek@ambiesense.com Scottish Government marinescotland@gov.scot [funder] International Council Exploration Sea (ICES) info@ices.dk [copyright holder] contributors: Helsinki Commission (HELCOM) secretariat@helcom.fi [copyright holder, funder, contributor] Arctic Monitoring Assessment Programme (AMAP) amap@amap.[copyright holder, funder, contributor] OSPAR Commission (OSPAR) secretariat@ospar.org [copyright holder, funder, contributor]","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Find the path for an information file — locate_information_file","title":"Find the path for an information file — locate_information_file","text":"Locates requested information file, searching information file path. requested file found required false, stops error.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find the path for an information file — locate_information_file","text":"","code":"locate_information_file(name, path)"},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find the path for an information file — locate_information_file","text":"name string: name file, e.g., thresholds_biota.csv path vector strings, directories search. information directory package automatically searched found file anywhere else","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/locate_information_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find the path for an information file — locate_information_file","text":"string, absolute path file, NULL file found anywhere.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Graphical summaries of an assessment — plot_assessment","title":"Graphical summaries of an assessment — plot_assessment","text":"Generates series assessment plots raw data, annual indices, . plots exported either png pdf files.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Graphical summaries of an assessment — plot_assessment","text":"","code":"plot_assessment(   assessment_obj,   subset = NULL,   output_dir = \".\",   file_type = c(\"data\", \"index\"),   file_format = c(\"png\", \"pdf\") )"},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Graphical summaries of an assessment — plot_assessment","text":"assessment_obj assessment object resulting call run_assessment subset optional vector specifying timeseries plotted. expression evaluated timeSeries component assessment_obj; use 'series' identify individual timeseries. output_dir output directory assessment plots (possibly supplied using 'file.path'). default working directory. output directory must already exist. file_type Specifies whether plots show raw data ('file_type = \"data\"'), annual indices summarising data year ('file_type = \"index\"'), (default) whether two files produced time series, one raw data one annual indices. file_format Whether files png (default) pdf.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/plot_assessment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Graphical summaries of an assessment — plot_assessment","text":"series png pdf files graphical summaries assessment. plots show fitted trends pointwise two-sided 90% confidence limits either raw data, indices summarising data year.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads contaminant data — read_contaminants","title":"Reads contaminant data — read_contaminants","text":"quick way reading contaminant data without station matching (using data_format == \"ICES)","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads contaminant data — read_contaminants","text":"","code":"read_contaminants(file, data_dir = \".\", info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads contaminant data — read_contaminants","text":"file file reference contaminant data. data_dir path directory holding contaminant data. Defaults working directory. info list containing least following two elements: compartment: \"biota\", \"sediment\" \"water\" data_format: \"ICES\" \"external\"","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_contaminants.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads contaminant data — read_contaminants","text":"data frame containing contaminant data.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Read HARSAT data — read_data","title":"Read HARSAT data — read_data","text":"Reads contaminant effects data, station dictionary various reference tables. data ICES webservice, matches data stations station dictionary.  also allows user set control parameters dictate assessment process.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read HARSAT data — read_data","text":"","code":"read_data(   compartment = c(\"biota\", \"sediment\", \"water\"),   purpose = c(\"OSPAR\", \"HELCOM\", \"AMAP\", \"custom\"),   contaminants,   stations,   QA,   data_dir = \".\",   data_format = c(\"ICES\", \"external\"),   info_files = list(),   info_dir = \".\",   extraction = NULL,   max_year = NULL,   oddity_dir = \"oddities\",   control = list() )"},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read HARSAT data — read_data","text":"compartment string: \"biota\", \"sediment\" \"water\" purpose string specifying whether use default set \"OSPAR\", \"HELCOM\", \"AMAP\" use customised setup \"custom\" contaminants file reference contaminant data stations file reference station data QA file reference QA data data_dir directory data files can found (sometimes supplied using 'file.path'). Defaults \".\"; .e. working directory. data_format string specifying whether data extracted ICES webservice (\"ICES\" - default) simplified format designed data sources (\"external\"). value \"ICES_old\" deprecated. info_files list files specifying reference tables override defaults. See examples. info_dir directory reference tables can found (sometimes supplied using 'file.path'). Defaults \".\"; .e. working directory extraction date saying extraction made. Optional. provided according ISO 8601; example, 29 February 2024 supplied \"2024-02-29\". contaminant data extracted ICES webservice download file name changed, extraction data taken contaminant file name. max_year integer giving last monitoring year included assessment. Data monitoring years max_year deleted. specified max_year taken last monitoring year contaminant data file. oddity_dir directory 'oddities' written (sometimes supplied using 'file.path'). directory (subdirectories) created already exist. control list control parameters override default values used run assessment. include reporting window; way data matched stations following ICES extraction; information reporting regions, . See Details.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read HARSAT data — read_data","text":"list following components: call function call. info list containing reference tables control parameters. data data frame containing contaminant (effects) data. external data, identical input data file apart extra empty columns added. ICES data, existing columns renamed (otherwise untouched) additional columns constructed. key ones : station_code code station station dictionary best matches data station_name name station species (biota) species based worms_accepted_name available speci_name otherwise filtration (water) whether sample filtered unfiltered based method_pretreatment retain logical indicating whether record retained previous ICES extraction protocol. example, retain FALSE vflag entry \"S\" suspect. Records retain == FALSE deleted later tidy_data stations","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads station dictionary — read_stations","title":"Reads station dictionary — read_stations","text":"Reads station dictionary","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads station dictionary — read_stations","text":"","code":"read_stations(file, data_dir = \".\", info)"},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads station dictionary — read_stations","text":"file file reference station dictionary. data_dir path directory holding station dictionary. Defaults working directory. info list containing least following two elements: compartment: \"biota\", \"sediment\" \"water\" data_format: \"ICES\" \"external\"","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/read_stations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads station dictionary — read_stations","text":"data frame containing station dictionary.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Assess timeseries for trends and status — run_assessment","title":"Assess timeseries for trends and status — run_assessment","text":"Fits model timeseries, test temporal trend compare thresholds. Need add lot details.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/run_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assess timeseries for trends and status — run_assessment","text":"","code":"run_assessment(   ctsm_ob,   subset = NULL,   AC = NULL,   get_AC_fn = NULL,   recent_trend = 20L,   parallel = FALSE,   ... )"},{"path":"http://osparcomm.github.io/HARSAT/reference/run_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assess timeseries for trends and status — run_assessment","text":"ctsm_ob HARSAT object resulting call create_timeSeries subset optional vector specifying timeseries assessed. Might used assessment done chunks size, refitting timeseries model converged. expression evaluated timeSeries component ctsm_ob; use 'series' identify individual timeseries. AC character vector identifying thresholds used status assessments. threshold reference table. Defaults NULL; .e. thresholds used. get_AC_fn optional function overrides get_AC_default. See details (need written). recent_trend integer giving number years used assessment recent trends. example, value 20 (default) consider trends last twenty year. parallel logical determines whether use parallel computation; default = FALSE. ... Extra arguments passed assessment_engine.  See details (need written).","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/tidy_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy the data — tidy_data","title":"Tidy the data — tidy_data","text":"Reduces size extraction removing redundant variables. ad-hoc changes usually made read_data simplify_data. output correct format create_timeSeries.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/tidy_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tidy the data — tidy_data","text":"","code":"tidy_data(ctsm_obj)"},{"path":"http://osparcomm.github.io/HARSAT/reference/tidy_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tidy the data — tidy_data","text":"ctsm_obj","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/update_assessment.html","id":null,"dir":"Reference","previous_headings":"","what":"Update timeseries assessments — update_assessment","title":"Update timeseries assessments — update_assessment","text":"Refits models particular timeseries, fits new models assessment done chunks.","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/update_assessment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update timeseries assessments — update_assessment","text":"","code":"update_assessment(ctsm_ob, subset = NULL, parallel = FALSE, ...)"},{"path":"http://osparcomm.github.io/HARSAT/reference/update_assessment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update timeseries assessments — update_assessment","text":"ctsm_ob HARSAT object resulting call run_assessment subset vector specifying timeseries assessements updated fit first time. expression evaluated timeSeries component ctsm_ob; use 'series' identify individual timeseries. parallel logical determines whether use parallel computation; default = FALSE. ... Extra arguments passed assessment_engine.  See details (need written).","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Write assessment summary to a csv file — write_summary_table","title":"Write assessment summary to a csv file — write_summary_table","text":"Creates data frame summarising assessment time series writes csv file. summary includes: meta-data monitoring location number years data time series fitted values last monitoring year associated upper one-sided 95% confidence limits trend assessments (p-values trend estimates) status assessments (thresholds) (optionally) symbology summarising trend (shape) status (colour) time series","code":""},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write assessment summary to a csv file — write_summary_table","text":"","code":"write_summary_table(   assessment_obj,   output_file = NULL,   output_dir = \".\",   export = TRUE,   determinandGroups = NULL,   classColour = NULL,   collapse_AC = NULL )"},{"path":"http://osparcomm.github.io/HARSAT/reference/write_summary_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write assessment summary to a csv file — write_summary_table","text":"assessment_obj assessment object resulting call run_assessment. output_file name output csv file. using NULL, file called biota_summary.csv, sediment_summary.csv water_summary.csv appropriate. default file written working directory. file name provided, path output file can also provided (e.g. using file.path). output_dir option can also used specify output file directory. output_dir output directory output_file. default working directory. file path provided output_file, appended output_dir. resulting output directory must already exist. export Logical. TRUE (default) writes summary table csv file. FALSE returns summary table R object (write csv file). collapse_AC","code":""}]
